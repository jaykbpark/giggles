# Cursor Rules - Clip iOS App (nw2025)

> **Project:** Clip (iOS Companion App for Meta Ray-Ban Glasses)
> **Platform:** iOS 26+ (SwiftUI with Liquid Glass)
> **Design Philosophy:** Apple-native, modern iOS 26 Liquid Glass aesthetic
> **Core Concept:** Invisible Interface for reviewing, searching, and reliving 60s video clips

**ðŸ“‹ Development Progress:** See [`frontend/PROGRESS.md`](frontend/PROGRESS.md) for current sprint, completed work, and integration points.

---

## Apple Documentation Reference

**IMPORTANT:** Always reference Apple's official iOS 26 documentation when implementing UI:

```
/Applications/Xcode.app/Contents/PlugIns/IDEIntelligenceChat.framework/Versions/A/Resources/AdditionalDocumentation/
```

Key files:
- `SwiftUI-Implementing-Liquid-Glass-Design.md` - Use `.glassEffect()` modifier
- `FoundationModels-Using-on-device-LLM-in-your-app.md` - On-device AI
- `Swift-Concurrency-Updates.md` - Modern async/await patterns

---

## Liquid Glass Design System (iOS 26)

### Basic Implementation
```swift
// Basic glass effect
Text("Hello").glassEffect()

// Custom shape
Text("Hello").glassEffect(in: .rect(cornerRadius: 16))

// Interactive glass (for buttons/tappable elements)
Button("Tap").glassEffect(.regular.interactive())

// Glass button styles
Button("Action") { }.buttonStyle(.glass)
Button("Important") { }.buttonStyle(.glassProminent)

// Multiple glass views (use container for morphing)
GlassEffectContainer(spacing: 20) {
    // views with .glassEffect()
}
```

### Glass Effect Rules
- Use `.glassEffect()` instead of `.ultraThinMaterial` for iOS 26
- Navigation bars, toolbars, tab bars â†’ automatic glass
- Floating UI elements â†’ `.glassEffect()`
- Interactive buttons â†’ `.glassEffect(.regular.interactive())`
- **DO NOT use for content areas** - glass is for chrome/navigation only

### Morphing Transitions
```swift
@Namespace private var namespace

GlassEffectContainer(spacing: 40.0) {
    HStack(spacing: 40.0) {
        Image(systemName: "icon")
            .glassEffect()
            .glassEffectID("uniqueID", in: namespace)
    }
}
```

---

## Swift 6.2 Concurrency

### Default MainActor Isolation
Swift 6.2 infers MainActor by default - no need to annotate everything with @MainActor.

### Background Work with @concurrent
```swift
nonisolated struct PhotoProcessor {
    @concurrent
    func process(data: Data) async -> ProcessedPhoto? { ... }
}

// Call with await
processedPhotos[item.id] = await PhotoProcessor().process(data: data)
```

### Isolated Conformances
```swift
extension StickerModel: @MainActor Exportable {
    func export() {
        photoProcessor.exportAsPNG()
    }
}
```

---

## Foundation Models (On-Device LLM)

### Check Availability First
```swift
private var model = SystemLanguageModel.default

switch model.availability {
case .available:
    // Show intelligence UI
case .unavailable(.deviceNotEligible):
    // Show alternative UI
case .unavailable(.appleIntelligenceNotEnabled):
    // Ask user to enable Apple Intelligence
case .unavailable(.modelNotReady):
    // Model downloading
case .unavailable(let other):
    // Unknown reason
}
```

### Basic Usage
```swift
let session = LanguageModelSession(instructions: "You are a helpful assistant.")
let response = try await session.respond(to: prompt)
print(response.content)
```

### Guided Generation (Structured Output)
```swift
@Generable(description: "Profile information")
struct Profile {
    var name: String
    @Guide(description: "Age of person", .range(0...120))
    var age: Int
}

let response = try await session.respond(to: "Generate a profile", generating: Profile.self)
print(response.content.name)
```

---

## Project Architecture

### Split-State Pipeline
1. **Capture (Edge)** - Save 60s video to Photo Library via PhotoKit
2. **Siphon (Background)** - Extract audio, upload to backend
3. **Intelligence (Cloud)** - Whisper â†’ Gemini â†’ MongoDB
4. **Search Loop** - Semantic search returns localIdentifiers, frontend rehydrates

### Design Tokens
- **Colors:** Use semantic colors (AppColors.background, .surface, .textPrimary, etc.)
- **Typography:** SF Pro (heroHeader 34pt bold, cardTitle 18pt semibold, body 16pt, metadata 14pt mono)
- **Layout:** Padding 20pt, Card radius 16pt, Search pill 30pt

### Project Structure
```
ClipApp/
â”œâ”€â”€ App/
â”‚   â”œâ”€â”€ ClipApp.swift
â”‚   â””â”€â”€ GlobalViewState.swift
â”œâ”€â”€ Core/
â”‚   â”œâ”€â”€ Navigation/RootView.swift
â”‚   â”œâ”€â”€ DesignSystem/ (Colors, Typography, Components)
â”‚   â””â”€â”€ Managers/ (Haptic, Photo, Camera)
â”œâ”€â”€ Features/
â”‚   â”œâ”€â”€ Feed/ (HomeFeedView, ClipCardView, StatusHeader)
â”‚   â””â”€â”€ Search/ (LiquidSearchOverlay, SearchResultsGrid)
â”œâ”€â”€ Services/ (APIService, MockData)
â””â”€â”€ Models/ (ClipMetadata, SearchResult)
```

---

## Wake Word Detection ("Clip That")

The app uses native iOS `SFSpeechRecognizer` for on-device wake word detection. When the user says "Clip That", it triggers the video clipping and returns the last 30 seconds of transcript.

### WakeWordDetector Usage
```swift
// 1. Start listening with your audio format from Meta SDK
wakeWordDetector.startListening(audioFormat: audioFormatFromMetaSDK)

// 2. Feed audio buffers as they arrive from the stream
wakeWordDetector.processAudioBuffer(buffer)

// 3. The onClipTriggered callback fires with the last 30 seconds of transcript
wakeWordDetector.onClipTriggered = { transcript in
    // transcript contains the last 30 seconds of speech (without "clip that")
    // Send to backend, trigger video clipping, etc.
    print("Transcript: \(transcript)")
}

// 4. Stop listening when done
wakeWordDetector.stopListening()

// Optional: Get current transcript manually
let recentTranscript = wakeWordDetector.getRecentTranscript()
```

### Key Details
- **Location:** `Core/Managers/WakeWordDetector.swift`
- **On-device:** Uses `requiresOnDeviceRecognition = true` (no network needed)
- **Transcript buffer:** Maintains rolling 30-second transcript buffer
- **Session restart:** Auto-restarts every 50 seconds (iOS recognition limit is ~1 minute)
- **Cooldown:** 2-second debounce to prevent duplicate triggers
- **UI indicator:** `GlassesStatusCard(isListening:)` shows pulsing mic when active
- **Published state:** `currentTranscript` shows live transcription

---

## Key Implementation Notes

1. **No TabView** - Search floats above content
2. **Haptics:** playSuccess() â†’ rigid double tap, playLight() â†’ soft impact, playError() â†’ heavy triple
3. **Search:** Semantic search via backend, results rendered as 2-column grid with thumbnails
4. **Status Header:** Green pulse dot + "RAY-BAN META â€¢ 82%" + aperture button

---

## Building and Deploying to iPhone

### Quick Deploy Command
To build and install directly to a connected iPhone from the command line:

```bash
cd frontend && xcodebuild -project nw2025.xcodeproj -scheme nw2025 -destination 'id=00008140-001534E83647001C' -configuration Debug -allowProvisioningUpdates build install
```

**Note:** Replace the device ID (`00008140-001534E83647001C`) with your iPhone's ID. To find your device ID:
```bash
xcrun xcodebuild -showdestinations -project frontend/nw2025.xcodeproj -scheme nw2025 2>&1 | grep "platform:iOS.*arch:arm64.*id:" | grep -v "Simulator"
```

### Code Signing Setup
- **Development Team:** `8DX2TD97D2` (Jay Park - Personal Team)
- **Bundle Identifier:** `me.park.jay.nw2025`
- **Signing Style:** Automatic (configured in project.pbxproj)
- The `-allowProvisioningUpdates` flag allows Xcode to automatically create/update provisioning profiles

### Requirements
- iPhone must have **Developer Mode enabled** (Settings â†’ Privacy & Security â†’ Developer Mode)
- Apple ID must be signed into Xcode (Xcode â†’ Settings â†’ Accounts)
- Free Apple ID works for personal development (apps expire after 7 days)

---

## Meta Wearables Device Access Toolkit (DAT)

**IMPORTANT:** Always reference this SDK documentation when developing features for Meta Ray-Ban glasses integration.

### SDK Overview
- Developer preview SDK for Meta AI glasses (Ray-Ban Meta)
- Enables video streaming, photo capture, and audio access
- General availability planned for 2026

### Installation (Swift Package Manager)
```swift
// In Xcode: File > Add Package Dependencies
// URL: https://github.com/facebook/meta-wearables-dat-ios
```

1. In Xcode, select **File** > **Add Package Dependencies...**
2. Search for `https://github.com/facebook/meta-wearables-dat-ios`
3. Select `meta-wearables-dat-ios`
4. Set the version to one of the available versions
5. Click **Add Package**
6. Select the target to which you want to add the packages
7. Click **Add Package**

### Key Capabilities
- **POV Camera:** Video streaming and photo capture from glasses
- **Microphone:** Open-ear microphone audio input
- **Audio Output:** Speakers for audio playback
- **Sensors:** On-device sensor access

### Analytics Opt-Out Configuration
To disable Meta analytics collection, add to `Info.plist`:

**Method 1: Xcode Info Tab**
1. Select app target in Project navigator
2. Go to **Info** tab â†’ **Custom iOS Target Properties**
3. Add `MWDAT` key (Dictionary)
4. Add `Analytics` key under MWDAT (Dictionary)
5. Add `OptOut` key under Analytics (Boolean) â†’ set to `YES`

**Method 2: Direct XML**
```xml
<key>MWDAT</key>
<dict>
    <key>Analytics</key>
    <dict>
        <key>OptOut</key>
        <true/>
    </dict>
</dict>
```

### Documentation Links
- **Blog:** https://developers.meta.com/blog/introducing-meta-wearables-device-access-toolkit/
- **Docs:** https://wearables.developer.meta.com/docs/develop
- **GitHub:** https://github.com/facebook/meta-wearables-dat-ios
- **Sample Code:** `samples/CameraAccess` in the GitHub repo

### Current Limitations (Developer Preview)
- Voice commands/AI assistant integration **not yet available**
- Publishing to public restricted to select partners only
- Internal distribution within orgs/teams is allowed
- Some features may change before general availability
